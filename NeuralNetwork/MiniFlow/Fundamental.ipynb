{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MengOonLee/BertelsmannAITrack/blob/master/NeuralNetwork/MiniFlow/Fundamental.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SnKVyduNYs6",
        "colab_type": "code",
        "outputId": "099d1319-5a40-4807-9eaf-ac8eacba8ec9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file ./miniflow.py\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class Node(object):\n",
        "  \"\"\"\n",
        "  Base class for nodes in the network.\n",
        "  Arguments:\n",
        "    `inbound_nodes`: A list of nodes with edges into this node.\n",
        "  \"\"\"\n",
        "  def __init__(self, inbound_nodes=[]):\n",
        "    \"\"\"\n",
        "    Node's constructor (runs when the object is instantiated).\n",
        "    Sets properties that all nodes need.\n",
        "    \"\"\"\n",
        "    # A list of nodes with edges into this node.\n",
        "    self.inbound_nodes = inbound_nodes\n",
        "    # The eventual value of this node.\n",
        "    # Set by running the forward() method.\n",
        "    self.value = None\n",
        "    # A list of nodes that this node outputs to.\n",
        "    self.outbound_nodes = []\n",
        "    # Keys are the inputs to this node and their values\n",
        "    # are the partials of this node with respect to that input.\n",
        "    self.gradients = {}\n",
        "    # Sets this node as an outbound node for all of this node's inputs.\n",
        "    for node in self.inbound_nodes:\n",
        "      node.outbound_nodes.append(self)\n",
        "\n",
        "  def forward(self):\n",
        "    \"\"\"\n",
        "    Every node that uses this class as a base class will need to define its\n",
        "    own `forward` method.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def backward(self):\n",
        "    \"\"\"\n",
        "    Every node that uses this class as a base class will need to define its \n",
        "    own `backward` method.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "class Input(Node):\n",
        "  \"\"\"\n",
        "  A generic input into the network.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    # The base class constructor has to run to set all the properties here.\n",
        "    # The most important property on an Input is value.\n",
        "    # self.value is set during `topological_sort` later\n",
        "    Node.__init__(self)\n",
        "\n",
        "  def forward(self):\n",
        "    # Do nothing because nothing is calculated.\n",
        "    pass\n",
        "\n",
        "  def backward(self):\n",
        "    # An Input node has no inputs so the gradient (derivative) is zero.\n",
        "    # The key `self` is reference to this object.\n",
        "    self.gradients = {self: 0}\n",
        "    # Weights and bias may be inputs, so you need to sum\n",
        "    # the gradient from output gradients.\n",
        "    for node in self.outbound_nodes:\n",
        "      self.gradients[self] += node.gradients[self] \n",
        "\n",
        "class Linear(Node):\n",
        "  \"\"\"\n",
        "  Represents a node that performs a linear transform\n",
        "  \"\"\"\n",
        "  def __init__(self, X, W, b):\n",
        "    # The base class (Node) constructor. Weights and bias\n",
        "    # are treated like inbound nodes.\n",
        "    Node.__init__(self, [X, W, b])\n",
        "\n",
        "  def forward(self):\n",
        "    \"\"\"\n",
        "    Performs the math behind a linear transform.\n",
        "    \"\"\"\n",
        "    X = self.inbound_nodes[0].value\n",
        "    W = self.inbound_nodes[1].value\n",
        "    b = self.inbound_nodes[2].value\n",
        "    self.value = np.dot(X, W) + b\n",
        "\n",
        "  def backward(self):\n",
        "    \"\"\"\n",
        "    Calculates the gradient based on the output values.\n",
        "    \"\"\"\n",
        "    # Initialize a partial for each of the inbound_nodes.\n",
        "    self.gradients = {node: np.zeros_like(node.value) \n",
        "      for node in self.inbound_nodes}\n",
        "    # Cycle through the outputs. The gradient will change depending\n",
        "    # on each output, so the gradients are summed over all outputs.\n",
        "    for node in self.outbound_nodes:\n",
        "      # Get the partial of the cost with respect to this node.\n",
        "      grad_cost = node.gradients[self]\n",
        "      # Set the partial of the loss with respect to this node's inputs.\n",
        "      self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, \n",
        "        self.inbound_nodes[1].value.T)\n",
        "      # Set the partial of the loss with respect to this node's weights.\n",
        "      self.gradients[self.inbound_nodes[1]] += np.dot(\n",
        "        self.inbound_nodes[0].value.T, grad_cost)\n",
        "      # Set the partial of the loss with respect to this node's bias.\n",
        "      self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, \n",
        "        keepdims=False)\n",
        "\n",
        "class Sigmoid(Node):\n",
        "  \"\"\"\n",
        "  Represents a node that performs the sigmoid activation function.\n",
        "  \"\"\"\n",
        "  def __init__(self, node):\n",
        "    # The base class constructor.\n",
        "    Node.__init__(self, [node])\n",
        "\n",
        "  def _sigmoid(self, x):\n",
        "    \"\"\"\n",
        "    This method is separate from `forward` because it \n",
        "    will be used later with `backward` as well.\n",
        "    `x`: A numpy array-like object.\n",
        "    \"\"\"\n",
        "    return 1. / (1. + np.exp(-x)) # the `.` ensures that `1` is a float\n",
        "\n",
        "  def forward(self):\n",
        "    \"\"\"\n",
        "    Perform the sigmoid function and set the value.\n",
        "    \"\"\"\n",
        "    input_value = self.inbound_nodes[0].value\n",
        "    self.value = self._sigmoid(input_value)\n",
        "\n",
        "  def backward(self):\n",
        "    \"\"\"\n",
        "    Calculates the gradient using the derivative of the sigmoid function.\n",
        "    \"\"\"\n",
        "    # Initialize the gradients to 0.\n",
        "    self.gradients = {node: np.zeros_like(node.value) \n",
        "      for node in self.inbound_nodes}\n",
        "    # Sum the partial with respect to the input over all the outputs.\n",
        "    for node in self.outbound_nodes:\n",
        "      grad_cost = node.gradients[self]\n",
        "      sigmoid = self.value\n",
        "      self.gradients[self.inbound_nodes[0]] += sigmoid * (1 - sigmoid) * grad_cost\n",
        "\n",
        "class MSE(Node):\n",
        "  \"\"\"\n",
        "  The mean squared error cost function.\n",
        "  Should be used as the last node for a network.\n",
        "  \"\"\"\n",
        "  def __init__(self, y, a):\n",
        "    # Call the base class constructor.\n",
        "    Node.__init__(self, [y, a])\n",
        "\n",
        "  def forward(self):\n",
        "    \"\"\"\n",
        "    Calculates the mean squared error.\n",
        "    NOTE: We reshape these to avoid possible matrix/vector broadcast errors.\n",
        "    For example, if we subtract an array of shape (3,) from an array of \n",
        "    shape (3,1) we get an array of shape (3,3) as the result when we want\n",
        "    an array of shape(3,1) instead.\n",
        "\n",
        "    Making both arrays (3,1) insures the result is (3,1) and does an\n",
        "    elementwise subtraction as expected.\n",
        "    \"\"\"\n",
        "    y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
        "    a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
        "\n",
        "    # Save the computed output for backward.\n",
        "    self.m = self.inbound_nodes[0].value.shape[0]\n",
        "    self.diff = y - a\n",
        "    self.value = np.mean(np.square(self.diff))\n",
        "\n",
        "  def backward(self):\n",
        "    \"\"\"\n",
        "    Calculates the gradient of the cost.\n",
        "    This is the final node of the network \n",
        "    so outbound nodes are not a concern.\n",
        "    \"\"\"\n",
        "    self.gradients[self.inbound_nodes[0]] = (2 / self.m) * self.diff\n",
        "    self.gradients[self.inbound_nodes[1]] = (-2 / self.m) * self.diff\n",
        "\n",
        "def topological_sort(feed_dict):\n",
        "  \"\"\"\n",
        "  Sort generic nodes in topological order using Kahn's Algorithm.\n",
        "  `feed_dict`: A dictionary where the key is a `Input` node \n",
        "  and the value is the respective value feed to that node.\n",
        "  Returns a list of sorted nodes.\n",
        "  \"\"\"\n",
        "\n",
        "  input_nodes = [n for n in feed_dict.keys()]\n",
        "\n",
        "  G = {}\n",
        "  nodes = [n for n in input_nodes]\n",
        "  while len(nodes) > 0:\n",
        "    n = nodes.pop(0)\n",
        "    if n not in G:\n",
        "      G[n] = {'in': set(), 'out': set()}\n",
        "    for m in n.outbound_nodes:\n",
        "      if m not in G:\n",
        "        G[m] = {'in': set(), 'out': set()}\n",
        "      G[n]['out'].add(m)\n",
        "      G[m]['in'].add(n)\n",
        "      nodes.append(m)\n",
        "\n",
        "  L = []\n",
        "  S = set(input_nodes)\n",
        "  while len(S) > 0:\n",
        "    n = S.pop()\n",
        "\n",
        "    if isinstance(n, Input):\n",
        "        n.value = feed_dict[n]\n",
        "\n",
        "    L.append(n)\n",
        "    for m in n.outbound_nodes:\n",
        "        G[n]['out'].remove(m)\n",
        "        G[m]['in'].remove(n)\n",
        "        # if no other incoming edges add to S\n",
        "        if len(G[m]['in'])==0:\n",
        "            S.add(m)\n",
        "  return L\n",
        "\n",
        "def forward_and_backward(graph):\n",
        "  \"\"\"\n",
        "  Performs a forward pass and a backward pass through a list of sorted nodes.\n",
        "  Arguments:\n",
        "    `graph`: The result of calling `topological_sort`.\n",
        "  \"\"\"\n",
        "  # Forward pass\n",
        "  for node in graph:\n",
        "    node.forward()\n",
        "\n",
        "  # Backward pass\n",
        "  for node in graph[::-1]:\n",
        "    node.backward()\n",
        "\n",
        "def sgd_update(trainables, learning_rate=1e-2):\n",
        "  \"\"\"\n",
        "  Updates the value of each trainable with SGD.\n",
        "  Arguments:\n",
        "    `trainables`: A list of `Input` Nodes representing weights/biases.\n",
        "    `learning_rate`: The learning rate.\n",
        "  \"\"\"\n",
        "  # Performs SGD\n",
        "  # Loop over the trainables\n",
        "  for t in trainables:\n",
        "    # Change the trainable's value by substracting the learning rate\n",
        "    # multiplied by the partial of the cost with respect to this trainable.\n",
        "    partial = t.gradients[t]\n",
        "    t.value -= learning_rate * partial"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing ./miniflow.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKnhTzPbHSRI",
        "colab_type": "text"
      },
      "source": [
        "## This script builds and runs a graph with miniflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC8ocTTOPVc7",
        "colab_type": "code",
        "outputId": "10208c27-a45c-42cf-80d6-650ef6d9e71a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\"\"\"\n",
        "In general, there's no restriction on the values that \n",
        "can be passed to an Input node.\n",
        "NOTE: The weights and biases are generated randomly.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.utils import shuffle, resample\n",
        "from miniflow import *\n",
        "\n",
        "# Load data\n",
        "data = load_boston()\n",
        "X_ = data['data']\n",
        "y_ = data['target']\n",
        "\n",
        "# Normalize data\n",
        "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
        "\n",
        "n_features = X_.shape[1]\n",
        "n_hidden = 10\n",
        "W1_ = np.random.randn(n_features, n_hidden)\n",
        "b1_ = np.zeros(n_hidden)\n",
        "W2_ = np.random.randn(n_hidden, 1)\n",
        "b2_ = np.zeros(1)\n",
        "\n",
        "# Neural network\n",
        "X, y = Input(), Input()\n",
        "W1, b1 = Input(), Input()\n",
        "W2, b2 = Input(), Input()\n",
        "\n",
        "l1 = Linear(X, W1, b1)\n",
        "s1 = Sigmoid(l1)\n",
        "l2 = Linear(s1, W2, b2)\n",
        "cost = MSE(y, l2)\n",
        "\n",
        "# The value of `Input` nodes will be set to values respectively.\n",
        "feed_dict = {\n",
        "  X: X_,\n",
        "  y: y_,\n",
        "  W1: W1_,\n",
        "  b1: b1_,\n",
        "  W2: W2_,\n",
        "  b2: b2_\n",
        "}\n",
        "\n",
        "epochs = 100\n",
        "# Total number of examples\n",
        "m = X_.shape[0]\n",
        "print(\"Total number of examples = {}\".format(m))\n",
        "\n",
        "batch_size = 11\n",
        "steps_per_epoch = m // batch_size\n",
        "\n",
        "# Sort the nodes with topological sort.\n",
        "graph = topological_sort(feed_dict)\n",
        "trainables = [W1, b1, W2, b2]\n",
        "\n",
        "for i in range(epochs):\n",
        "  loss = 0\n",
        "  for j in range(steps_per_epoch):\n",
        "    # Randomly sample a batch of examples\n",
        "    X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
        "\n",
        "    # Reset value of X and y Inputs\n",
        "    X.value = X_batch\n",
        "    y.value = y_batch\n",
        "\n",
        "    forward_and_backward(graph)\n",
        "\n",
        "    sgd_update(trainables)\n",
        "\n",
        "    loss += graph[-1].value\n",
        "\n",
        "  print(\"Epoch: {}, Loss: {:.3f}\".format(i+1, loss/steps_per_epoch))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of examples = 506\n",
            "Epoch: 1, Loss: 104.280\n",
            "Epoch: 2, Loss: 33.462\n",
            "Epoch: 3, Loss: 28.881\n",
            "Epoch: 4, Loss: 24.323\n",
            "Epoch: 5, Loss: 23.334\n",
            "Epoch: 6, Loss: 18.288\n",
            "Epoch: 7, Loss: 16.756\n",
            "Epoch: 8, Loss: 19.553\n",
            "Epoch: 9, Loss: 13.866\n",
            "Epoch: 10, Loss: 12.461\n",
            "Epoch: 11, Loss: 14.619\n",
            "Epoch: 12, Loss: 12.018\n",
            "Epoch: 13, Loss: 13.583\n",
            "Epoch: 14, Loss: 10.815\n",
            "Epoch: 15, Loss: 10.406\n",
            "Epoch: 16, Loss: 13.418\n",
            "Epoch: 17, Loss: 10.091\n",
            "Epoch: 18, Loss: 12.014\n",
            "Epoch: 19, Loss: 12.380\n",
            "Epoch: 20, Loss: 9.063\n",
            "Epoch: 21, Loss: 9.172\n",
            "Epoch: 22, Loss: 9.404\n",
            "Epoch: 23, Loss: 10.138\n",
            "Epoch: 24, Loss: 8.852\n",
            "Epoch: 25, Loss: 9.983\n",
            "Epoch: 26, Loss: 8.385\n",
            "Epoch: 27, Loss: 10.245\n",
            "Epoch: 28, Loss: 7.141\n",
            "Epoch: 29, Loss: 8.272\n",
            "Epoch: 30, Loss: 7.855\n",
            "Epoch: 31, Loss: 9.545\n",
            "Epoch: 32, Loss: 9.211\n",
            "Epoch: 33, Loss: 9.291\n",
            "Epoch: 34, Loss: 8.458\n",
            "Epoch: 35, Loss: 6.589\n",
            "Epoch: 36, Loss: 8.361\n",
            "Epoch: 37, Loss: 7.176\n",
            "Epoch: 38, Loss: 9.749\n",
            "Epoch: 39, Loss: 7.375\n",
            "Epoch: 40, Loss: 9.447\n",
            "Epoch: 41, Loss: 8.947\n",
            "Epoch: 42, Loss: 8.488\n",
            "Epoch: 43, Loss: 8.395\n",
            "Epoch: 44, Loss: 7.619\n",
            "Epoch: 45, Loss: 11.206\n",
            "Epoch: 46, Loss: 9.632\n",
            "Epoch: 47, Loss: 8.301\n",
            "Epoch: 48, Loss: 7.456\n",
            "Epoch: 49, Loss: 8.255\n",
            "Epoch: 50, Loss: 7.935\n",
            "Epoch: 51, Loss: 7.146\n",
            "Epoch: 52, Loss: 7.505\n",
            "Epoch: 53, Loss: 7.434\n",
            "Epoch: 54, Loss: 6.836\n",
            "Epoch: 55, Loss: 6.606\n",
            "Epoch: 56, Loss: 7.470\n",
            "Epoch: 57, Loss: 6.797\n",
            "Epoch: 58, Loss: 8.569\n",
            "Epoch: 59, Loss: 7.698\n",
            "Epoch: 60, Loss: 7.968\n",
            "Epoch: 61, Loss: 8.284\n",
            "Epoch: 62, Loss: 7.932\n",
            "Epoch: 63, Loss: 8.635\n",
            "Epoch: 64, Loss: 8.037\n",
            "Epoch: 65, Loss: 7.699\n",
            "Epoch: 66, Loss: 7.667\n",
            "Epoch: 67, Loss: 8.021\n",
            "Epoch: 68, Loss: 6.667\n",
            "Epoch: 69, Loss: 5.795\n",
            "Epoch: 70, Loss: 8.789\n",
            "Epoch: 71, Loss: 7.985\n",
            "Epoch: 72, Loss: 7.676\n",
            "Epoch: 73, Loss: 5.654\n",
            "Epoch: 74, Loss: 7.097\n",
            "Epoch: 75, Loss: 5.593\n",
            "Epoch: 76, Loss: 5.798\n",
            "Epoch: 77, Loss: 6.716\n",
            "Epoch: 78, Loss: 7.973\n",
            "Epoch: 79, Loss: 6.804\n",
            "Epoch: 80, Loss: 6.119\n",
            "Epoch: 81, Loss: 6.408\n",
            "Epoch: 82, Loss: 6.079\n",
            "Epoch: 83, Loss: 6.495\n",
            "Epoch: 84, Loss: 7.757\n",
            "Epoch: 85, Loss: 7.622\n",
            "Epoch: 86, Loss: 7.082\n",
            "Epoch: 87, Loss: 6.787\n",
            "Epoch: 88, Loss: 8.139\n",
            "Epoch: 89, Loss: 6.516\n",
            "Epoch: 90, Loss: 6.962\n",
            "Epoch: 91, Loss: 6.740\n",
            "Epoch: 92, Loss: 6.129\n",
            "Epoch: 93, Loss: 6.769\n",
            "Epoch: 94, Loss: 7.085\n",
            "Epoch: 95, Loss: 7.115\n",
            "Epoch: 96, Loss: 6.728\n",
            "Epoch: 97, Loss: 6.319\n",
            "Epoch: 98, Loss: 5.887\n",
            "Epoch: 99, Loss: 6.059\n",
            "Epoch: 100, Loss: 6.413\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}