{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MengOonLee/BertelsmannAITrack/blob/master/NeuralNetwork/MiniFlow/Fundamental.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SnKVyduNYs6",
        "colab_type": "code",
        "outputId": "577b146d-cbb0-4901-d2f9-22df4dac4493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%file ./miniflow.py\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class Node(object):\n",
        "  \"\"\"\n",
        "  Base class for nodes in the network.\n",
        "  Arguments:\n",
        "    `inbound_nodes`: A list of nodes with edges into this node.\n",
        "  \"\"\"\n",
        "  def __init__(self, inbound_nodes=[]):\n",
        "    \"\"\"\n",
        "    Node's constructor (runs when the object is instantiated).\n",
        "    Sets properties that all nodes need.\n",
        "    \"\"\"\n",
        "    # A list of nodes with edges into this node.\n",
        "    self.inbound_nodes = inbound_nodes\n",
        "    # The eventual value of this node.\n",
        "    # Set by running the forward() method.\n",
        "    self.value = None\n",
        "    # A list of nodes that this node outputs to.\n",
        "    self.outbound_nodes = []\n",
        "    # Keys are the inputs to this node and their values\n",
        "    # are the partials of this node with respect to that input.\n",
        "    self.gradients = {}\n",
        "    # Sets this node as an outbound node for all of this node's inputs.\n",
        "    for node in self.inbound_nodes:\n",
        "      node.outbound_nodes.append(self)\n",
        "\n",
        "  def forward(self):\n",
        "    \"\"\"\n",
        "    Every node that uses this class as a base class will need to define its\n",
        "    own `forward` method.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def backward(self):\n",
        "    \"\"\"\n",
        "    Every node that uses this class as a base class will need to define its \n",
        "    own `backward` method.\n",
        "    \"\"\"\n",
        "    raise NotImplementedError\n",
        "\n",
        "class Input(Node):\n",
        "  \"\"\"\n",
        "  A generic input into the network.\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    # The base class constructor has to run to set all the properties here.\n",
        "    # The most important property on an Input is value.\n",
        "    # self.value is set during `topological_sort` later\n",
        "    Node.__init__(self)\n",
        "\n",
        "  def forward(self):\n",
        "    # Do nothing because nothing is calculated.\n",
        "    pass\n",
        "\n",
        "  def backward(self):\n",
        "    # An Input node has no inputs so the gradient (derivative) is zero.\n",
        "    # The key `self` is reference to this object.\n",
        "    self.gradients = {self: 0}\n",
        "    # Weights and bias may be inputs, so you need to sum\n",
        "    # the gradient from output gradients.\n",
        "    for node in self.outbound_nodes:\n",
        "      grad_cost = node.gradients[self]\n",
        "      self.gradients[self] += grad_cost * 1 \n",
        "\n",
        "class Linear(Node):\n",
        "  \"\"\"\n",
        "  Represents a node that performs a linear transform\n",
        "  \"\"\"\n",
        "  def __init__(self, X, W, b):\n",
        "    # The base class (Node) constructor. Weights and bias\n",
        "    # are treated like inbound nodes.\n",
        "    Node.__init__(self, [X, W, b])\n",
        "\n",
        "  def forward(self):\n",
        "    \"\"\"\n",
        "    Performs the math behind a linear transform.\n",
        "    \"\"\"\n",
        "    X = self.inbound_nodes[0].value\n",
        "    W = self.inbound_nodes[1].value\n",
        "    b = self.inbound_nodes[2].value\n",
        "    self.value = np.dot(X, W) + b\n",
        "\n",
        "  def backward(self):\n",
        "    \"\"\"\n",
        "    Calculates the gradient based on the output values.\n",
        "    \"\"\"\n",
        "    # Initialize a partial for each of the inbound_nodes.\n",
        "    self.gradients = {node: np.zeros_like(node.value) \n",
        "      for node in self.inbound_nodes}\n",
        "    # Cycle through the outputs. The gradient will change depending\n",
        "    # on each output, so the gradients are summed over all outputs.\n",
        "    for node in self.outbound_nodes:\n",
        "      # Get the partial of the cost with respect to this node.\n",
        "      grad_cost = node.gradients[self]\n",
        "      # Set the partial of the loss with respect to this node's inputs.\n",
        "      self.gradients[self.inbound_nodes[0]] += np.dot(grad_cost, \n",
        "        self.inbound_nodes[1].value.T)\n",
        "      # Set the partial of the loss with respect to this node's weights.\n",
        "      self.gradients[self.inbound_nodes[1]] += np.dot(\n",
        "        self.inbound_nodes[0].value.T, grad_cost)\n",
        "      # Set the partial of the loss with respect to this node's bias.\n",
        "      self.gradients[self.inbound_nodes[2]] += np.sum(grad_cost, axis=0, \n",
        "        keepdims=False)\n",
        "      \n",
        "\n",
        "class Sigmoid(Node):\n",
        "  \"\"\"\n",
        "  Represents a node that performs the sigmoid activation function.\n",
        "  \"\"\"\n",
        "  def __init__(self, node):\n",
        "    # The base class constructor.\n",
        "    Node.__init__(self, [node])\n",
        "\n",
        "  def _sigmoid(self, x):\n",
        "    \"\"\"\n",
        "    This method is separate from `forward` because it \n",
        "    will be used later with `backward` as well.\n",
        "    `x`: A numpy array-like object.\n",
        "    \"\"\"\n",
        "    return 1. / (1. + np.exp(-x)) # the `.` ensures that `1` is a float\n",
        "\n",
        "  def forward(self):\n",
        "    \"\"\"\n",
        "    Perform the sigmoid function and set the value.\n",
        "    \"\"\"\n",
        "    input_value = self.inbound_nodes[0].value\n",
        "    self.value = self._sigmoid(input_value)\n",
        "\n",
        "  def backward(self):\n",
        "    \"\"\"\n",
        "    Calculates the gradient using the derivative of the sigmoid function.\n",
        "    \"\"\"\n",
        "    # Initialize the gradients to 0.\n",
        "    self.gradients = {node: np.zeros_like(node.value) \n",
        "      for node in self.inbound_nodes}\n",
        "    # Cycle through the outputs. The gradient will change depending\n",
        "    # on each output, so the gradients are summed over all outputs.\n",
        "    for node in self.outbound_nodes:\n",
        "      # Get the partial of the cost with respect to this node.\n",
        "      grad_cost = node.gradients[self]\n",
        "      # Set the gradients property to the gradients with respect to each input.\n",
        "      sigmoid = self.value\n",
        "      self.gradients[self.inbound_nodes[0]] += sigmoid * (1 - sigmoid) * grad_cost\n",
        "\n",
        "class MSE(Node):\n",
        "  \"\"\"\n",
        "  The mean squared error cost function.\n",
        "  Should be used as the last node for a network.\n",
        "  \"\"\"\n",
        "  def __init__(self, y, a):\n",
        "    # Call the base class constructor.\n",
        "    Node.__init__(self, [y, a])\n",
        "\n",
        "  def forward(self):\n",
        "    \"\"\"\n",
        "    Calculates the mean squared error.\n",
        "    NOTE: We reshape these to avoid possible matrix/vector broadcast errors.\n",
        "    For example, if we subtract an array of shape (3,) from an array of \n",
        "    shape (3,1) we get an array of shape (3,3) as the result when we want\n",
        "    an array of shape(3,1) instead.\n",
        "\n",
        "    Making both arrays (3,1) insures the result is (3,1) and does an\n",
        "    elementwise subtraction as expected.\n",
        "    \"\"\"\n",
        "    y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
        "    a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
        "\n",
        "    # Save the computed output for backward.\n",
        "    self.m = self.inbound_nodes[0].value.shape[0]\n",
        "    self.diff = y - a\n",
        "    self.value = np.mean(np.square(self.diff))\n",
        "\n",
        "  def backward(self):\n",
        "    \"\"\"\n",
        "    Calculates the gradient of the cost.\n",
        "    This is the final node of the network \n",
        "    so outbound nodes are not a concern.\n",
        "    \"\"\"\n",
        "    self.gradients[self.inbound_nodes[0]] = (2 / self.m) * self.diff\n",
        "    self.gradients[self.inbound_nodes[1]] = (-2 / self.m) * self.diff\n",
        "\n",
        "def topological_sort(feed_dict):\n",
        "  \"\"\"\n",
        "  Sort generic nodes in topological order using Kahn's Algorithm.\n",
        "  `feed_dict`: A dictionary where the key is a `Input` node \n",
        "  and the value is the respective value feed to that node.\n",
        "  Returns a list of sorted nodes.\n",
        "  \"\"\"\n",
        "\n",
        "  input_nodes = [n for n in feed_dict.keys()]\n",
        "\n",
        "  G = {}\n",
        "  nodes = [n for n in input_nodes]\n",
        "  while len(nodes) > 0:\n",
        "    n = nodes.pop(0)\n",
        "    if n not in G:\n",
        "      G[n] = {'in': set(), 'out': set()}\n",
        "    for m in n.outbound_nodes:\n",
        "      if m not in G:\n",
        "        G[m] = {'in': set(), 'out': set()}\n",
        "      G[n]['out'].add(m)\n",
        "      G[m]['in'].add(n)\n",
        "      nodes.append(m)\n",
        "\n",
        "  L = []\n",
        "  S = set(input_nodes)\n",
        "  while len(S) > 0:\n",
        "    n = S.pop()\n",
        "\n",
        "    if isinstance(n, Input):\n",
        "        n.value = feed_dict[n]\n",
        "\n",
        "    L.append(n)\n",
        "    for m in n.outbound_nodes:\n",
        "        G[n]['out'].remove(m)\n",
        "        G[m]['in'].remove(n)\n",
        "        # if no other incoming edges add to S\n",
        "        if len(G[m]['in'])==0:\n",
        "            S.add(m)\n",
        "  return L\n",
        "\n",
        "def forward_and_backward(graph):\n",
        "  \"\"\"\n",
        "  Performs a forward pass and a backward pass through a list of sorted nodes.\n",
        "  Arguments:\n",
        "    `graph`: The result of calling `topological_sort`.\n",
        "  \"\"\"\n",
        "  # Forward pass\n",
        "  for node in graph:\n",
        "    node.forward()\n",
        "\n",
        "  # Backward pass\n",
        "  for node in graph[::-1]:\n",
        "    node.backward()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing ./miniflow.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKnhTzPbHSRI",
        "colab_type": "text"
      },
      "source": [
        "## This script builds and runs a graph with miniflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qC8ocTTOPVc7",
        "colab_type": "code",
        "outputId": "e9f59787-c61d-4292-941b-965fe654acc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "\"\"\"\n",
        "In general, there's no restriction on the values that \n",
        "can be passed to an Input node.\n",
        "NOTE: The weights and biases are generated randomly.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.utils import shuffle, resample\n",
        "from miniflow import *\n",
        "\n",
        "# Load data\n",
        "data = load_boston()\n",
        "X_ = data['data']\n",
        "y_ = data['target']\n",
        "\n",
        "# Normalize data\n",
        "X_ = (X_ - np.mean(X_, axis=0)) / np.std(X_, axis=0)\n",
        "\n",
        "n_features = X_.shape[1]\n",
        "n_hidden = 10\n",
        "W1_ = np.random.randn(n_features, n_hidden)\n",
        "b1_ = np.zeros(n_hidden)\n",
        "W2_ = np.random.randn(n_hidden, 1)\n",
        "b2_ = np.zeros(1)\n",
        "\n",
        "# Neural network\n",
        "X, y = Input(), Input()\n",
        "W1, b1 = Input(), Input()\n",
        "W2, b2 = Input(), Input()\n",
        "\n",
        "l1 = Linear(X, W1, b1)\n",
        "s1 = Sigmoid(l1)\n",
        "l2 = Linear(s1, W2, b2)\n",
        "cost = MSE(y, l2)\n",
        "\n",
        "# The value of `Input` nodes will be set to values respectively.\n",
        "feed_dict = {\n",
        "  X: X_,\n",
        "  y: y_,\n",
        "  W1: W1_,\n",
        "  b1: b1_,\n",
        "  W2: W2_,\n",
        "  b2: b2_\n",
        "}\n",
        "\n",
        "epochs = 10\n",
        "# Total number of examples\n",
        "m = X_.shape[0]\n",
        "print(\"Total number of examples = {}\".format(m))\n",
        "\n",
        "batch_size = 11\n",
        "steps_per_epoch = m // batch_size\n",
        "\n",
        "# Sort the nodes with topological sort.\n",
        "graph = topological_sort(feed_dict)\n",
        "trainables = [W1, b1, W2, b2]\n",
        "\n",
        "# Step 4\n",
        "for i in range(epochs):\n",
        "  loss = 0\n",
        "  for j in range(steps_per_epoch):\n",
        "    # Step 1\n",
        "    # Randomly sample a batch of examples\n",
        "    X_batch, y_batch = resample(X_, y_, n_samples=batch_size)\n",
        "\n",
        "# forward and backward pass\n",
        "forward_and_backward(graph)\n",
        "# return the gradients for each Input\n",
        "gradients = [t.gradients[t] for t in [X, y, W, b]]\n",
        "\n",
        "# NOTE: because topological_sort sets the values for the `Input` nodes we could also access\n",
        "# the value for x with x.value.\n",
        "print(\"output = {} \".format(gradients))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output = [array([[-3.34017280e-05, -5.01025919e-05],\n",
            "       [-6.68040138e-05, -1.00206021e-04]]), array([[0.9999833],\n",
            "       [1.9999833]]), array([[5.01028709e-05],\n",
            "       [1.00205742e-04]]), array([-5.01028709e-05])] \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}