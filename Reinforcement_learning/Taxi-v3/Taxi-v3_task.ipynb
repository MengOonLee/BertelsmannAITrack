{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4e09e0-d650-4539-ac36-886a48a6a50d",
   "metadata": {},
   "source": [
    "# OpenAI gym's Taxi-v3 task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d189874-7720-4b54-8c23-09b024b0f2e0",
   "metadata": {},
   "source": [
    "For this coding exercise, we will use OpenAI gym's [Taxi-v3](https://gym.openai.com/envs/Taxi-v3/) environment to design an algorithm to teach a taxi agent to navigate a small gridworld. There are 4 locations (labeled by different letters) and our job is to pick up the passenger at one location and drop him off in another. We will receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\n",
    "\n",
    "The Taxi Problem   \n",
    "from \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\" by Tom Dietterich\n",
    "\n",
    "Description:  \n",
    "There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
    "\n",
    "Observations:  \n",
    "There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations.\n",
    "Note that there are 400 states that can actually be reached during an episode. The missing states correspond to situations in which the passenger is at the same location as their destination, as this typically signals the end of an episode.\n",
    "Four additional states can be observed right after a successful episodes, when both the passenger and the taxi are at the destination.\n",
    "This gives a total of 404 reachable discrete states.\n",
    "\n",
    "Passenger locations:\n",
    "- 0: R(ed)\n",
    "- 1: G(reen)\n",
    "- 2: Y(ellow)\n",
    "- 3: B(lue)\n",
    "- 4: in taxi\n",
    "\n",
    "Destinations:\n",
    "- 0: R(ed)\n",
    "- 1: G(reen)\n",
    "- 2: Y(ellow)\n",
    "- 3: B(lue)\n",
    "\n",
    "Actions:  \n",
    "There are 6 discrete deterministic actions:\n",
    "- 0: move south\n",
    "- 1: move north\n",
    "- 2: move east\n",
    "- 3: move west\n",
    "- 4: pickup passenger\n",
    "- 5: drop off passenger\n",
    "\n",
    "Rewards:  \n",
    "There is a default per-step reward of -1, except for delivering the passenger, which is +20, or executing \"pickup\" and \"drop-off\" actions illegally, which is -10.\n",
    "\n",
    "Rendering:  \n",
    "- blue: passenger\n",
    "- magenta: destination\n",
    "- yellow: empty taxi\n",
    "- green: full taxi\n",
    "- other letters (R, G, Y and B): locations for passengers and destinations\n",
    "\n",
    "State space is represented by:  \n",
    "(taxi_row, taxi_col, passenger_location, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd51ae28-52b3-41a8-8204-6718bb937b87",
   "metadata": {},
   "source": [
    "### Specify the environment, and explore the state & action spaces\n",
    "\n",
    "Create an instance of the [Taxi-V3](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py) environment that has a discreate state and action spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c891ac2-b912-4da6-b795-ec711f691517",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# create the environment and set random seed\n",
    "env = gym.make('Taxi-v3')\n",
    "env.seed(505)\n",
    "# print the state and action spaces\n",
    "print('State space:', env.observation_space)\n",
    "print('Action space:', env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06af5ec8-68a7-4501-ab01-633ef9601229",
   "metadata": {},
   "source": [
    "### Specify the interaction\n",
    "\n",
    "It might be helpful to get some experience with the output that is returned as the agent interacts with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995638c6-9108-40ba-b31e-394877a3faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import math, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def interact(env, agent, num_episodes=20000, window=100):\n",
    "    \"\"\"\n",
    "    Monitor agent's performance.\n",
    "    \n",
    "    Params\n",
    "    ==========\n",
    "    - env: instance of OpenAI Gym's Taxi-v3 environment.\n",
    "    - agent: instance of class Agent (see Agent class for details).\n",
    "    - num_episodes: number of episodes of agent-environment interaction.\n",
    "    - window: number of episodes to consider when calculating average rewards.\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    - samp_rewards: list containing final sample reward/episode.\n",
    "    \"\"\"\n",
    "    # initialize best average reward\n",
    "    best_avg_reward = -math.inf\n",
    "    # initialize sample rewards\n",
    "    samp_rewards = []\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        # begin the episode, observe S\n",
    "        state = env.reset()\n",
    "        # initialize sample reward\n",
    "        samp_reward = 0\n",
    "        # roll out steps until done\n",
    "        done = False\n",
    "        while not done:\n",
    "            # agent selects an action\n",
    "            action = agent.select_action(state)\n",
    "            # agent performs the selected action, observe R, S'\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # agent performs internal updates based on previous experience\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            # update the sample reward\n",
    "            samp_reward += reward\n",
    "            # update the state (s <- s') to next time step\n",
    "            state = next_state\n",
    "        # save final sample reward\n",
    "        samp_rewards.append(samp_reward)\n",
    "        # get average reward over number of consecutive episodes\n",
    "        avg_rewards = pd.Series(samp_rewards).rolling(window).mean()\n",
    "        # update best average reward\n",
    "        best_avg_reward = np.max(avg_rewards)\n",
    "        # monitor progress & performance\n",
    "        print(\"\\rEpisode {}/{} || Best average reward(last {} episodes) {}\\n\".format(\n",
    "            i_episode, num_episodes, window, best_avg_reward), end='')\n",
    "        sys.stdout.flush()\n",
    "        # check if task is solved (according to OpenAI Gym)\n",
    "        if best_avg_reward>=9.7:\n",
    "            print(f\"\\nEnvironment solved in {i_episode} episodes\", end='')\n",
    "        # watch agent interacts with environment.\n",
    "#         print(env.render(mode='ansi'))\n",
    "    # plot performance\n",
    "    _, ax = plt.subplots(figsize=(10, 5))\n",
    "    # plot sample reward obtained per episode as well as \n",
    "    # average reward over number consecutive episodes.\n",
    "    ax.plot(samp_rewards, label=\"Sample\")\n",
    "    ax.plot(avg_rewards, label=\"Average\")\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    ax.set_title(f\"Sample & Average(last {window} episodes) Rewards\")\n",
    "    ax.set_ylabel(\"Reward\")\n",
    "    ax.set_xlabel(\"Episode #\")\n",
    "    plt.show()\n",
    "    \n",
    "    return samp_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40d5216b-6447-4125-a41e-367348a7431a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0123456789"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "  \n",
    "for i in range(10):\n",
    "    print(i, end ='', flush = True)\n",
    "#     sys.stdout.flush()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be95b98-e236-4658-b4da-fe6af04c8286",
   "metadata": {},
   "source": [
    "### TD Control: Q-Learning\n",
    "\n",
    "Provided below is an Q-Learning agent. The algorithm updates the dictionary of `Q` where `Q[s][a]` is the estimated action value corresponding to state `s` and action `a`.\n",
    "\n",
    "\\begin{align} \\label{eq:1}\n",
    "Q(S_t, A_t) &\\leftarrow (1 - \\alpha) Q(S_t, A_t) + \n",
    "\\alpha [R_{t+1} + \\gamma \\max_{a} Q(S_{t+1}, a)], \\tag{1} \\\\\n",
    "&\\quad a \\in A(s) \\notag\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aec3da1-695f-481c-8983-369b1394ed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"\n",
    "    Q-learning agent that act on the OpenAI Gym's Taxi-v3 environment.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, alpha=0.02, gamma=0.99, epsilon=1.0, \n",
    "        epsilon_decay_rate=0.9995, min_epsilon=.01):\n",
    "        \"\"\"\n",
    "        Initialize agent.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        - env: instance of OpenAI Gym's Taxi-v3 environment.\n",
    "        \"\"\"\n",
    "        # enviroment info\n",
    "        self.nA = env.nA\n",
    "        # learning parameters:\n",
    "        # learning rate\n",
    "        self.alpha = alpha\n",
    "        # discount factor\n",
    "        self.gamma = gamma\n",
    "        # initial exploration rate\n",
    "        self.epsilon = self.initial_epsilon = epsilon\n",
    "        # how quickly should epsilon decrease\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate\n",
    "        self.min_epsilon = min_epsilon\n",
    "        # initialize action-value function (empty dictionary of arrays)\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
    "     \n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Given the state, select an action.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        - state: the current state of the environment.\n",
    "\n",
    "        Returns\n",
    "        =======\n",
    "        - action: an integer, compatible with the task's action space.\n",
    "        \"\"\"\n",
    "        # reset epsilon for a new episode, \n",
    "        # decrease exploration rate gradually\n",
    "        self.epsilon *= self.epsilon_decay_rate\n",
    "        self.epsilon = max(self.epsilon, self.min_epsilon)\n",
    "        # get epsilon-greedy policy\n",
    "        policy_s = np.ones(self.nA) * self.epsilon / self.nA\n",
    "        policy_s[np.argmax(self.Q[state])] = \\\n",
    "            1 - self.epsilon + (self.epsilon / self.nA)\n",
    "        return np.random.choice(np.arange(self.nA), p=policy_s)\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Update the agent's knowledge, using the most recently sampled tuple.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        - state: the previous state of the environment.\n",
    "        - action: the agent's previous choice of action.\n",
    "        - reward: last reward received.\n",
    "        - next_state: the current state of the environment.\n",
    "        - done: whether the episode is complete (True or False).\n",
    "        \"\"\"\n",
    "        if not done:\n",
    "            self.Q[state][action] += self.alpha * (reward + \n",
    "                self.gamma * np.max(self.Q[next_state]) - self.Q[state][action])\n",
    "        if done:\n",
    "            self.Q[state][action] += self.alpha * (reward - self.Q[state][action])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb67b52-3af7-48dc-b2ec-14d0a9dbf7e1",
   "metadata": {},
   "source": [
    "Execute the code cell below to play **Taxi-v3** with a TD policy in 20000 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff71f06-9bb2-4ad4-8352-f423fe9024bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(env)\n",
    "samp_rewards = interact(env, agent, num_episodes=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55052128-0ac2-4e9a-afb1-8eceea68a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def interact(env, agent, num_episodes=20000, window=100):\n",
    "    \"\"\" Monitor agent's performance.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "    - env: instance of OpenAI Gym's Taxi-v1 environment\n",
    "    - agent: instance of class Agent (see Agent.py for details)\n",
    "    - num_episodes: number of episodes of agent-environment interaction\n",
    "    - window: number of episodes to consider when calculating average rewards\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "    - avg_rewards: deque containing average rewards\n",
    "    - best_avg_reward: largest value in the avg_rewards deque\n",
    "    \"\"\"\n",
    "    # initialize average rewards\n",
    "    avg_rewards = deque(maxlen=num_episodes)\n",
    "    # initialize best average reward\n",
    "    best_avg_reward = -math.inf\n",
    "    # initialize monitor for most recent rewards\n",
    "    samp_rewards = deque(maxlen=window)\n",
    "    # for each episode\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # begin the episode\n",
    "        state = env.reset()\n",
    "        # initialize the sampled reward\n",
    "        samp_reward = 0\n",
    "        while True:\n",
    "            # agent selects an action\n",
    "            action = agent.select_action(state)\n",
    "            # agent performs the selected action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # agent performs internal updates based on sampled experience\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            # update the sampled reward\n",
    "            samp_reward += reward\n",
    "            # update the state (s <- s') to next time step\n",
    "            state = next_state\n",
    "            if done:\n",
    "                # save final sampled reward\n",
    "                samp_rewards.append(samp_reward)\n",
    "                break\n",
    "        if (i_episode >= 100):\n",
    "            # get average reward from last 100 episodes\n",
    "            avg_reward = np.mean(samp_rewards)\n",
    "            # append to deque\n",
    "            avg_rewards.append(avg_reward)\n",
    "            # update best average reward\n",
    "            if avg_reward > best_avg_reward:\n",
    "                best_avg_reward = avg_reward\n",
    "        # monitor progress\n",
    "        print(\"\\rEpisode {}/{} || Best average reward {}\".format(i_episode, num_episodes, best_avg_reward), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        # check if task is solved (according to OpenAI Gym)\n",
    "        if best_avg_reward >= 9.7:\n",
    "            print('\\nEnvironment solved in {} episodes.'.format(i_episode), end=\"\")\n",
    "            break\n",
    "        if i_episode == num_episodes: print('\\n')\n",
    "    return avg_rewards, best_avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baaf533-7462-43f0-b141-6d53fd98f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, nA=6):\n",
    "        \"\"\"\n",
    "        Initialize agent.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        nA: integer\n",
    "            Number of actions available to the agent\n",
    "        \"\"\"\n",
    "        self.nA = nA\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Given the state, select an action.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        state: integer\n",
    "            The current state of the environment\n",
    "\n",
    "        Returns\n",
    "        =======\n",
    "        action: integer \n",
    "            The task's action space\n",
    "        \"\"\"\n",
    "        return np.random.choice(self.nA)\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Update the agent's knowledge, using the most recently sampled tuple.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        state: integer\n",
    "            The previous state of the environment.\n",
    "        action: integer\n",
    "            The agent's previous choice of action.\n",
    "        reward: integer \n",
    "            Last reward received.\n",
    "        next_state: integer\n",
    "            The current state of the environment.\n",
    "        done: logic\n",
    "            Whether the episode is complete (True or False)\n",
    "        \"\"\"\n",
    "        self.Q[state][action] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3494d2a3-70cd-4889-b0e0-d78151183b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "from monitor import interact\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('Taxi-v2')\n",
    "agent = Agent()\n",
    "avg_rewards, best_avg_reward = interact(env, agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
