{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4e09e0-d650-4539-ac36-886a48a6a50d",
   "metadata": {},
   "source": [
    "# OpenAI gym's Taxi-v3 task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d189874-7720-4b54-8c23-09b024b0f2e0",
   "metadata": {},
   "source": [
    "For this coding exercise, we will use OpenAI gym's [Taxi-v3](https://gym.openai.com/envs/Taxi-v3/) environment to design an algorithm to teach a taxi agent to navigate a small gridworld. There are 4 locations (labeled by different letters) and our job is to pick up the passenger at one location and drop him off in another. We will receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\n",
    "\n",
    "The Taxi Problem   \n",
    "from \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\" by Tom Dietterich\n",
    "\n",
    "Description:  \n",
    "There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
    "\n",
    "Observations:  \n",
    "There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations.\n",
    "Note that there are 400 states that can actually be reached during an episode. The missing states correspond to situations in which the passenger is at the same location as their destination, as this typically signals the end of an episode.\n",
    "Four additional states can be observed right after a successful episodes, when both the passenger and the taxi are at the destination.\n",
    "This gives a total of 404 reachable discrete states.\n",
    "\n",
    "Passenger locations:\n",
    "- 0: R(ed)\n",
    "- 1: G(reen)\n",
    "- 2: Y(ellow)\n",
    "- 3: B(lue)\n",
    "- 4: in taxi\n",
    "\n",
    "Destinations:\n",
    "- 0: R(ed)\n",
    "- 1: G(reen)\n",
    "- 2: Y(ellow)\n",
    "- 3: B(lue)\n",
    "\n",
    "Actions:  \n",
    "There are 6 discrete deterministic actions:\n",
    "- 0: move south\n",
    "- 1: move north\n",
    "- 2: move east\n",
    "- 3: move west\n",
    "- 4: pickup passenger\n",
    "- 5: drop off passenger\n",
    "\n",
    "Rewards:  \n",
    "There is a default per-step reward of -1, except for delivering the passenger, which is +20, or executing \"pickup\" and \"drop-off\" actions illegally, which is -10.\n",
    "\n",
    "Rendering:  \n",
    "- blue: passenger\n",
    "- magenta: destination\n",
    "- yellow: empty taxi\n",
    "- green: full taxi\n",
    "- other letters (R, G, Y and B): locations for passengers and destinations\n",
    "\n",
    "State space is represented by:  \n",
    "(taxi_row, taxi_col, passenger_location, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c88b8d2-96c3-4da2-b987-511fdd69a142",
   "metadata": {},
   "source": [
    "### Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd51ae28-52b3-41a8-8204-6718bb937b87",
   "metadata": {},
   "source": [
    "### Specify the environment, and explore the state & action spaces\n",
    "Create an instance of the [Taxi-V3](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py) environment that has a discreate state and action spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c891ac2-b912-4da6-b795-ec711f691517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Discrete(500)\n",
      "Action space: Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Create the environment and set random seed\n",
    "env = gym.make('Taxi-v3')\n",
    "env.seed(505)\n",
    "print('State space:', env.observation_space)\n",
    "print('Action space:', env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06af5ec8-68a7-4501-ab01-633ef9601229",
   "metadata": {},
   "source": [
    "It might be helpful to get some experience with the output that is returned as the agent interacts with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "995638c6-9108-40ba-b31e-394877a3faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import math, sys\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def interact(env, agent, num_episodes=20000, window=100):\n",
    "    \"\"\"\n",
    "    Monitor agent's performance.\n",
    "    \n",
    "    Parameters\n",
    "    ==========\n",
    "    env: class object\n",
    "        Instance of OpenAI Gym's Taxi-v3 environment.\n",
    "    agent: class object\n",
    "        Instance of class Agent (see Agent class for details).\n",
    "    num_episodes: integer\n",
    "        Number of episodes of agent-environment interaction.\n",
    "    window: integer\n",
    "        Number of episodes to consider when calculating average rewards.\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    avg_rewards: deque_like\n",
    "        Deque containing average rewards.\n",
    "    best_avg: float\n",
    "        Largest value in the avg_rewards deque.\n",
    "    \"\"\"\n",
    "    # Initialize average rewards\n",
    "    avg_rewards = deque(maxlen=num_episodes)\n",
    "    # Initialize best average reward\n",
    "    best_avg_reward = -math.inf\n",
    "    # Initialize monitor for most recent rewards\n",
    "    samp_rewards = deque(maxlen=window)\n",
    "    # For each episode\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        # Begin the episode\n",
    "        state = env.reset()\n",
    "        # Initialize the sampled reward\n",
    "        samp_reward = 0\n",
    "        # Roll out steps until done\n",
    "        done = False\n",
    "        while not done:\n",
    "            # Agent selects an action\n",
    "            action = agent.select_action(state)\n",
    "            # Agent performs the selected action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # Agent performs internal updates based on sampled experience\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            # Update the sampled reward\n",
    "            samp_reward += reward\n",
    "            # update the state (s <- s') to next time step\n",
    "            state = next_state\n",
    "            # Watch agent interacts with environment.\n",
    "            clear_output(wait=True)\n",
    "            print(f\"\\rEpisode {i_episode}/{num_episodes}:\\n\", end=\"\")\n",
    "            print(env.render(mode='ansi'))\n",
    "            print(f\"Cumulative reward/episode: {samp_reward}\")\n",
    "#             sleep(.1)\n",
    "        # Save final sampled reward\n",
    "        samp_rewards.append(samp_reward)\n",
    "        # Monitor progress over 100 consecutive episodes\n",
    "        if (i_episode>=100):\n",
    "            # Get average reward from last 100 episodes.\n",
    "            avg_reward = np.mean(samp_rewards)\n",
    "            # Append to deque \n",
    "            avg_rewards.append(avg_reward)\n",
    "            # Update best average reward\n",
    "            if avg_reward > best_avg_reward:\n",
    "                best_avg_reward = avg_reward\n",
    "        # Print best average reward\n",
    "        print(f\"\\rEpisode {i_episode}/{num_episodes} \\\n",
    "            || Best average reward {best_avg_reward}\", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        # Check if task is solved (according to OpenAI Gym)\n",
    "        if best_avg_reward>=9.7:\n",
    "            print(f\"\\nEnvironment solved in {i_episode} episodes\", end=\"\")\n",
    "        \n",
    "    return avg_rewards, best_avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aec3da1-695f-481c-8983-369b1394ed7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, nA=6):\n",
    "        \"\"\"\n",
    "        Initialize agent.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        nA: integer\n",
    "            Number of actions available to the agent\n",
    "        \"\"\"\n",
    "        self.nA = nA\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Given the state, select an action.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        state: integer\n",
    "            The current state of the environment\n",
    "\n",
    "        Returns\n",
    "        =======\n",
    "        action: integer \n",
    "            The task's action space\n",
    "        \"\"\"\n",
    "        return np.random.choice(self.nA)\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Update the agent's knowledge, using the most recently sampled tuple.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        state: integer\n",
    "            The previous state of the environment.\n",
    "        action: integer\n",
    "            The agent's previous choice of action.\n",
    "        reward: integer \n",
    "            Last reward received.\n",
    "        next_state: integer\n",
    "            The current state of the environment.\n",
    "        done: logic\n",
    "            Whether the episode is complete (True or False)\n",
    "        \"\"\"\n",
    "        self.Q[state][action] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb67b52-3af7-48dc-b2ec-14d0a9dbf7e1",
   "metadata": {},
   "source": [
    "Execute the code cell below to play **Taxi-v3** with a random policy in 3 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff71f06-9bb2-4ad4-8352-f423fe9024bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 99/100:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "\n",
      "Cumulative reward/episode: -70\n"
     ]
    }
   ],
   "source": [
    "random_agent = Agent()\n",
    "avg_rewards, best_avg_reward = interact(\n",
    "    env, random_agent, num_episodes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c90a263-c321-4b48-8eb8-8df8f8a0fb90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([-778.64])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55052128-0ac2-4e9a-afb1-8eceea68a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "def interact(env, agent, num_episodes=20000, window=100):\n",
    "    \"\"\" Monitor agent's performance.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "    - env: instance of OpenAI Gym's Taxi-v1 environment\n",
    "    - agent: instance of class Agent (see Agent.py for details)\n",
    "    - num_episodes: number of episodes of agent-environment interaction\n",
    "    - window: number of episodes to consider when calculating average rewards\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "    - avg_rewards: deque containing average rewards\n",
    "    - best_avg_reward: largest value in the avg_rewards deque\n",
    "    \"\"\"\n",
    "    # initialize average rewards\n",
    "    avg_rewards = deque(maxlen=num_episodes)\n",
    "    # initialize best average reward\n",
    "    best_avg_reward = -math.inf\n",
    "    # initialize monitor for most recent rewards\n",
    "    samp_rewards = deque(maxlen=window)\n",
    "    # for each episode\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # begin the episode\n",
    "        state = env.reset()\n",
    "        # initialize the sampled reward\n",
    "        samp_reward = 0\n",
    "        while True:\n",
    "            # agent selects an action\n",
    "            action = agent.select_action(state)\n",
    "            # agent performs the selected action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # agent performs internal updates based on sampled experience\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            # update the sampled reward\n",
    "            samp_reward += reward\n",
    "            # update the state (s <- s') to next time step\n",
    "            state = next_state\n",
    "            if done:\n",
    "                # save final sampled reward\n",
    "                samp_rewards.append(samp_reward)\n",
    "                break\n",
    "        if (i_episode >= 100):\n",
    "            # get average reward from last 100 episodes\n",
    "            avg_reward = np.mean(samp_rewards)\n",
    "            # append to deque\n",
    "            avg_rewards.append(avg_reward)\n",
    "            # update best average reward\n",
    "            if avg_reward > best_avg_reward:\n",
    "                best_avg_reward = avg_reward\n",
    "        # monitor progress\n",
    "        print(\"\\rEpisode {}/{} || Best average reward {}\".format(i_episode, num_episodes, best_avg_reward), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        # check if task is solved (according to OpenAI Gym)\n",
    "        if best_avg_reward >= 9.7:\n",
    "            print('\\nEnvironment solved in {} episodes.'.format(i_episode), end=\"\")\n",
    "            break\n",
    "        if i_episode == num_episodes: print('\\n')\n",
    "    return avg_rewards, best_avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9baaf533-7462-43f0-b141-6d53fd98f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, nA=6):\n",
    "        \"\"\"\n",
    "        Initialize agent.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        nA: integer\n",
    "            Number of actions available to the agent\n",
    "        \"\"\"\n",
    "        self.nA = nA\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Given the state, select an action.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        state: integer\n",
    "            The current state of the environment\n",
    "\n",
    "        Returns\n",
    "        =======\n",
    "        action: integer \n",
    "            The task's action space\n",
    "        \"\"\"\n",
    "        return np.random.choice(self.nA)\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Update the agent's knowledge, using the most recently sampled tuple.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        state: integer\n",
    "            The previous state of the environment.\n",
    "        action: integer\n",
    "            The agent's previous choice of action.\n",
    "        reward: integer \n",
    "            Last reward received.\n",
    "        next_state: integer\n",
    "            The current state of the environment.\n",
    "        done: logic\n",
    "            Whether the episode is complete (True or False)\n",
    "        \"\"\"\n",
    "        self.Q[state][action] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3494d2a3-70cd-4889-b0e0-d78151183b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "from monitor import interact\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('Taxi-v2')\n",
    "agent = Agent()\n",
    "avg_rewards, best_avg_reward = interact(env, agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
