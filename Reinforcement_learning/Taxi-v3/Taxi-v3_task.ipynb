{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e4e09e0-d650-4539-ac36-886a48a6a50d",
   "metadata": {},
   "source": [
    "# OpenAI gym's Taxi-v3 task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d189874-7720-4b54-8c23-09b024b0f2e0",
   "metadata": {},
   "source": [
    "For this coding exercise, we will use OpenAI gym's [Taxi-v3](https://gym.openai.com/envs/Taxi-v3/) environment to design an algorithm to teach a taxi agent to navigate a small gridworld. There are 4 locations (labeled by different letters) and our job is to pick up the passenger at one location and drop him off in another. We will receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\n",
    "\n",
    "The Taxi Problem   \n",
    "from \"Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition\" by Tom Dietterich\n",
    "\n",
    "Description:  \n",
    "There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
    "\n",
    "Observations:  \n",
    "There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations.\n",
    "Note that there are 400 states that can actually be reached during an episode. The missing states correspond to situations in which the passenger is at the same location as their destination, as this typically signals the end of an episode.\n",
    "Four additional states can be observed right after a successful episodes, when both the passenger and the taxi are at the destination.\n",
    "This gives a total of 404 reachable discrete states.\n",
    "\n",
    "Passenger locations:\n",
    "- 0: R(ed)\n",
    "- 1: G(reen)\n",
    "- 2: Y(ellow)\n",
    "- 3: B(lue)\n",
    "- 4: in taxi\n",
    "\n",
    "Destinations:\n",
    "- 0: R(ed)\n",
    "- 1: G(reen)\n",
    "- 2: Y(ellow)\n",
    "- 3: B(lue)\n",
    "\n",
    "Actions:  \n",
    "There are 6 discrete deterministic actions:\n",
    "- 0: move south\n",
    "- 1: move north\n",
    "- 2: move east\n",
    "- 3: move west\n",
    "- 4: pickup passenger\n",
    "- 5: drop off passenger\n",
    "\n",
    "Rewards:  \n",
    "There is a default per-step reward of -1, except for delivering the passenger, which is +20, or executing \"pickup\" and \"drop-off\" actions illegally, which is -10.\n",
    "\n",
    "Rendering:  \n",
    "- blue: passenger\n",
    "- magenta: destination\n",
    "- yellow: empty taxi\n",
    "- green: full taxi\n",
    "- other letters (R, G, Y and B): locations for passengers and destinations\n",
    "\n",
    "State space is represented by:  \n",
    "(taxi_row, taxi_col, passenger_location, destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c88b8d2-96c3-4da2-b987-511fdd69a142",
   "metadata": {},
   "source": [
    "### Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14c444c3-0f98-42a6-86c3-859039bad437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import defaultdict, deque\n",
    "import sys\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd51ae28-52b3-41a8-8204-6718bb937b87",
   "metadata": {},
   "source": [
    "### Specify the Environment, and explore the state and action spaces\n",
    "Create an instance of the [Taxi-V3](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py) environment that has a discreate state and action spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c891ac2-b912-4da6-b795-ec711f691517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State space: Discrete(500)\n",
      "Action space: Discrete(6)\n"
     ]
    }
   ],
   "source": [
    "# Create the environment and set random seed\n",
    "env = gym.make('Taxi-v3')\n",
    "env.seed(505)\n",
    "print('State space:', env.observation_space)\n",
    "print('Action space:', env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06af5ec8-68a7-4501-ab01-633ef9601229",
   "metadata": {},
   "source": [
    "It might be helpful to get some experience with the output that is returned as the agent interacts with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "168ad976-d0f6-4c8b-9a9a-dcd14ca8b0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_frames(frames):\n",
    "    \"\"\"\n",
    "    Watch agent interacts with environment in each rendered frame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    frames: array_like\n",
    "        A sequence of frame containing agent interacts with enviroment.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Animation of frames\n",
    "    \"\"\"\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb67b52-3af7-48dc-b2ec-14d0a9dbf7e1",
   "metadata": {},
   "source": [
    "Play **Taxi-v3** with a random policy in 200 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec51cedf-0170-4f27-b92c-65f8f37fef59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "\n",
      "Timestep: 200\n",
      "Action: 3\n",
      "State: 371\n",
      "Reward: -1\n",
      "Final score: -713\n"
     ]
    }
   ],
   "source": [
    "with env:\n",
    "    # begin the episode\n",
    "    state = env.reset()\n",
    "    # initialize the sampled reward & frames\n",
    "    samp_reward = 0\n",
    "    frames = []\n",
    "    while True:\n",
    "        # agent selects an action\n",
    "        action = env.action_space.sample()\n",
    "        # agent performs the selected action\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        # Put each rendered frame into dict for animation\n",
    "        # update the sampled reward & frames\n",
    "        samp_reward += reward\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'action': action,\n",
    "            'state': state,\n",
    "            'reward': reward\n",
    "        })\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "print_frames(frames)\n",
    "print(f\"Final score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d03f18-503c-4081-8858-dcd7313d0aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37abb17-dbe1-4d7a-b9fe-3f6458982a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self, nA=6):\n",
    "        \"\"\" Initialize agent.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        - nA: number of actions available to the agent\n",
    "        \"\"\"\n",
    "        self.nA = nA\n",
    "        self.Q = defaultdict(lambda: np.zeros(self.nA))\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        \"\"\" Given the state, select an action.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        - state: the current state of the environment\n",
    "        \n",
    "        Returns\n",
    "        =======\n",
    "        - action: an integer, compatible with the task's action space\n",
    "        \"\"\"       \n",
    "        return np.random.choice(self.nA)\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\" Update the agent's knowledge, using the most recently sampled tuple.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "        - state: the previous state of the environment\n",
    "        - action: the agent's previous choice of action\n",
    "        - reward: last reward received\n",
    "        - next_state: the current state of the environment\n",
    "        - done: whether the episode is complete (True or False)\n",
    "        \"\"\"\n",
    "        self.Q[state][action] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956aa2cc-3b95-4ea5-a94d-32a1b01c22a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact(env, agent, num_episodes=20000, window=100):\n",
    "    \"\"\" Monitor agent's performance.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "    - env: instance of OpenAI Gym's Taxi-v3 environment\n",
    "    - agent: instance of class Agent (see Agent for details)\n",
    "    - num_episodes: number of episodes of agent-environment interaction\n",
    "    - window: number of episodes to consider when calculating average rewards\n",
    "    \n",
    "    Returns\n",
    "    =======\n",
    "    - avg_rewards: deque containing average rewards\n",
    "    - best_avg_reward: largest value in the avg_rewards deque\n",
    "    \"\"\"\n",
    "    # initialize average rewards\n",
    "    avg_rewards = deque(maxlen=num_episodes)\n",
    "    # initialize best average reward\n",
    "    best_avg_reward = -math.inf\n",
    "    # initialize monitor for most recent rewards\n",
    "    samp_rewards = deque(maxlen=window)\n",
    "    # for each episode\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        # begin the episode\n",
    "        state = env.reset()\n",
    "        # initialize the sampled reward\n",
    "        samp_reward = 0\n",
    "        while True:\n",
    "            # agent selects an action\n",
    "            action = agent.select_action(state)\n",
    "            # agent performs the selected action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            # agent performs internal updates based on sampled experience\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            # update the sampled reward\n",
    "            samp_reward += reward\n",
    "            # update the state (s <- s') to next time step\n",
    "            state = next_state\n",
    "            if done:\n",
    "                # save final sampled reward\n",
    "                samp_rewards.append(samp_reward)\n",
    "                break\n",
    "        if (i_episode >= 100):\n",
    "            # get average reward from last 100 episodes\n",
    "            avg_reward = np.mean(samp_rewards)\n",
    "            # append to deque\n",
    "            avg_rewards.append(avg_reward)\n",
    "            # update best average reward\n",
    "            if avg_reward > best_avg_reward:\n",
    "                best_avg_reward = avg_reward\n",
    "        # monitor progress\n",
    "        print('\\rEpisode {}/{} || Best average reward {}'.format(\n",
    "            i_episode, num_episodes, best_avg_reward), end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        # check if task is solved (according to OpenAI Gym)\n",
    "        if best_avg_reward >= 9.7:\n",
    "            print('\\nEnvironment solved in {} episodes.'.format(\n",
    "                i_episode), end=\"\")\n",
    "            break\n",
    "        if i_episode == num_episodes:\n",
    "            print('\\n')\n",
    "        if i_episode % 5000 == 0:\n",
    "            env.render()\n",
    "            print('Reward = {}'.format(reward))\n",
    "    return avg_rewards, best_avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2893910-74b9-4a2f-bc01-25169c05731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent()\n",
    "avg_rewards, best_avg_reward = interact(env, agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
